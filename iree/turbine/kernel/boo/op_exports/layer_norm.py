# Copyright 2025 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

import argparse
from enum import IntEnum
from typing import Any, Sequence, Optional
import torch
import math
from functools import cached_property
from ..exports.signature import OpSignature, ModeBase
from ..exports.parser import OpCLIParser


class Mode(ModeBase, IntEnum):
    """Mode selector for layer normalization, with each gradient being its own mode."""

    FORWARD = 1
    INPUT_BACKWARD = 2
    WEIGHT_BACKWARD = 4
    BIAS_BACKWARD = 8
    FULL_BACKWARD = INPUT_BACKWARD | WEIGHT_BACKWARD | BIAS_BACKWARD


class LayerNormSignature(OpSignature):
    """Layer normalization signature that provides information for launching specific kernels."""

    input_shape: list[int]
    normalized_shape: list[int]
    eps: float
    elementwise_affine: bool
    bias: bool
    dtype: torch.dtype
    mode: Mode

    def __init__(
        self,
        *,
        input_shape: Sequence[int],
        normalized_shape: Sequence[int],
        eps: float = 1e-5,
        elementwise_affine: bool = True,
        bias: bool = True,
        dtype=torch.bfloat16,
        mode: str | Mode = Mode.FORWARD,
        forwarded_args_dtype: torch.dtype | None = None,
        use_aten: bool = True,
    ):
        if (
            len(normalized_shape) > len(input_shape)
            or input_shape[-len(normalized_shape) :] != normalized_shape
        ):
            raise ValueError(
                f"expected normalized shape to contain the trailing dimensions of the input shape, got {input_shape=}, {normalized_shape=}"
            )

        self.input_shape = input_shape
        self.normalized_shape = normalized_shape
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        self.bias = bias
        self.dtype = dtype
        self.mode = Mode.parse(mode)
        self.forwarded_args_dtype = forwarded_args_dtype or dtype
        self.use_aten = use_aten

    @property
    def output_shape(self) -> list[int]:
        return self.input_shape

    @staticmethod
    def get(
        input: torch.Tensor,
        normalized_shape: int | Sequence[int] | torch.Size,
        weight: Optional[torch.Tensor] = None,
        bias: Optional[torch.Tensor] = None,
        mode: Mode = Mode.FORWARD,
        **kwargs,
    ) -> "LayerNormSignature":
        """Creates a signature to run on the given PyTorch tensors.

        Weight and bias, if provided, must have the shape defined in
        `normalized_shape`. Bias may only be provided if weight is.
        """

        normalized_shape_list = (
            list(normalized_shape)
            if not isinstance(normalized_shape, int)
            else [normalized_shape]
        )
        assert (
            weight is None or list(weight.shape) == normalized_shape_list
        ), f"Expected the weight tensor to have {normalized_shape=}, got {weight.shape}."
        assert (
            bias is None or list(bias.shape) == normalized_shape_list
        ), f"Expected the bias tensor to have {normalized_shape=}, got {bias.shape}."
        assert (
            bias is None or weight is not None
        ), f"Expected the bias tensor to be provided only if the weight tensor is."
        if weight is not None:
            assert weight.dtype == input.dtype
        if bias is not None:
            assert weight.dtype == bias.dtype
        return LayerNormSignature(
            input_shape=list(input.shape),
            normalized_shape=normalized_shape_list,
            elementwise_affine=weight is not None,
            bias=bias is not None,
            dtype=input.dtype,
            mode=mode,
            **kwargs,
        )

    @cached_property
    def func_name(self) -> str:
        # XXX: this must exactly match the name generated by the compiler, e.g.
        # layer_norm_3d_bfloat16_42x12x45x13_w_b.
        name_items = [
            "layer_norm",
            f"{len(self.normalized_shape)}d",
            str(self.dtype).removeprefix("torch."),
            str(self.forwarded_args_dtype).removeprefix("torch."),
            self.mode.name.lower(),
            "x".join(str(i) for i in self.input_shape),
            "w" if self.elementwise_affine is not None else "",
            "b" if self.bias is not None else "",
            "aten" if self.use_aten else "",
        ]
        return "_".join(name_items)

    @property
    def aggregate_shape(self) -> list[int]:
        """Shape for aggregates that preserves leading unit dimensions."""
        return self.input_shape[
            : len(self.input_shape) - len(self.normalized_shape)
        ] + [1 for _ in range(len(self.normalized_shape))]

    @property
    def is_forward(self) -> bool:
        return self.mode == Mode.FORWARD

    def make_signature_copy_for_forward(self) -> "LayerNormSignature":
        kwargs = self.as_init_kwargs()
        kwargs["mode"] = Mode.FORWARD
        return LayerNormSignature(**kwargs)

    def get_arg_index_for_backward(self) -> int | None:
        assert not self.is_forward
        match self.mode:
            case Mode.INPUT_BACKWARD:
                return 0
            case Mode.WEIGHT_BACKWARD:
                return 1
            case Mode.BIAS_BACKWARD:
                return 2

    def arrange_backward_launch_args(
        self,
        forward_args: tuple[torch.Tensor, ...],
        forward_results: tuple[torch.Tensor, ...],
    ) -> tuple[torch.Tensor]:
        input = forward_args[0]
        # TODO: is this possible at this level?
        weight = forward_args[1] if len(forward_args) > 1 else None
        bias = forward_args[2] if len(forward_args) > 2 else None
        _, mean, rstd = forward_results
        if self.mode == Mode.INPUT_BACKWARD:
            return (input, weight, mean, rstd)
        if self.mode == Mode.WEIGHT_BACKWARD:
            return (input, weight, mean, rstd)
        if self.mode == Mode.BIAS_BACKWARD:
            return (input, weight, bias, mean, rstd)
        if self.mode == Mode.FULL_BACKWARD:
            return (input, weight, bias, mean, rstd)
        assert False, "Unsupported mode."

    def as_init_kwargs(self) -> dict[str, Any]:
        return {
            "input_shape": self.input_shape,
            "normalized_shape": self.normalized_shape,
            "eps": self.eps,
            "elementwise_affine": self.elementwise_affine,
            "bias": self.bias,
            "dtype": self.dtype,
            "mode": self.Mode,
            "forwarded_args_dtype": self.forwarded_args_dtype,
            "use_aten": self.use_aten,
        }

    def get_output_size(self) -> int:
        if self.mode == Mode.FORWARD:
            return (
                math.prod(self.output_shape) + 2 * math.prod(self.aggregate_shape)
            ) * int(self.dtype.itemsize)
        if self.mode == Mode.INPUT_BACKWARD:
            return math.prod(self.output_shape) * int(self.dtype.itemsize)
        if self.mode == Mode.BIAS_BACKWARD or self.mode == Mode.WEIGHT_BACKWARD:
            return math.prod(self.normalized_shape) * int(self.dtype.itemsize)

    def get_nn_module(self, use_custom: bool) -> torch.nn.Module:
        # TODO: this is specific to conv and may need to be refactored further
        assert use_custom, "skipping use_custom not supported for layernorm"
        if self.mode == Mode.FORWARD:
            return LayerNormForward(self)
        if self.mode == Mode.INPUT_BACKWARD:
            return LayerNormBackwardInput(self)
        if self.mode == Mode.WEIGHT_BACKWARD:
            return LayerNormBackwardWeight(self)
        if self.mode == Mode.BIAS_BACKWARD:
            return LayerNormBackwardBias(self)
        if self.mode == Mode.FULL_BACKWARD:
            return LayerNormBackwardFull(self)
        assert False, f"Unknown mode: {self.mode.name}."

    def get_sample_args(
        self,
        *,
        device: str | torch.device | None = None,
        splat_value: int | float | None = None,
        seed: Optional[int] = None,
    ) -> tuple[torch.Tensor, ...]:
        gen = torch.Generator(device=device)
        if seed:
            gen = gen.manual_seed(seed)

        def get(shape: Sequence[int]) -> torch.Tensor:
            if splat_value is not None:
                return torch.ones(shape, dtype=self.dtype, device=device) * splat_value
            return torch.randn(shape, generator=gen, dtype=self.dtype, device=device)

        if self.mode == Mode.FORWARD:
            # (x, w?, b?)
            args = [get(self.input_shape)]
            if self.elementwise_affine:
                args.append(get(self.normalized_shape))
            if self.bias:
                args.append(get(self.normalized_shape))
            return tuple(args)
        if self.mode == Mode.INPUT_BACKWARD:
            # (dLdy, input, weight, mean, rstd)
            return (
                get(self.output_shape),
                get(self.input_shape),
                get(self.normalized_shape),
                get(self.aggregate_shape).to(dtype=self.forwarded_args_dtype),
                get(self.aggregate_shape).to(dtype=self.forwarded_args_dtype),
            )
        if self.mode == Mode.WEIGHT_BACKWARD:
            # (dLdy, input, weight, mean, rstd)
            return (
                get(self.output_shape),
                get(self.input_shape),
                get(self.normalized_shape),
                get(self.aggregate_shape).to(dtype=self.forwarded_args_dtype),
                get(self.aggregate_shape).to(dtype=self.forwarded_args_dtype),
            )
        if self.mode == Mode.BIAS_BACKWARD:
            # (dLdy, input, weight, bias, mean, rstd)
            return (
                get(self.output_shape),
                get(self.input_shape),
                get(self.normalized_shape),
                get(self.normalized_shape),
                get(self.aggregate_shape).to(dtype=self.forwarded_args_dtype),
                get(self.aggregate_shape).to(dtype=self.forwarded_args_dtype),
            )
        if self.mode == Mode.FULL_BACKWARD:
            return (
                get(self.output_shape),
                get(self.input_shape),
                get(self.normalized_shape) if self.elementwise_affine else None,
                get(self.normalized_shape) if self.bias else None,
                get(self.aggregate_shape).to(dtype=self.forwarded_args_dtype),
                get(self.aggregate_shape).to(dtype=self.forwarded_args_dtype),
            )
        raise ValueError(f"Unknown mode: {self.mode}")


class LayerNormForward(torch.nn.Module):
    """Module implementing the forward mode layer norm."""

    def __init__(self, signature: LayerNormSignature):
        super().__init__()
        self.normalized_shape = signature.normalized_shape
        self.eps = signature.eps
        self.forwarded_args_dtype = signature.forwarded_args_dtype
        self.use_aten = signature.use_aten
        self.normalized_dim = list(
            range(len(signature.input_shape))[-len(self.normalized_shape) :]
        )

    def forward(
        self,
        input: torch.Tensor,
        weight: torch.Tensor | None = None,
        bias: torch.Tensor | None = None,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Compute entrypoint.

        Returns the layer norm result followed by tensors containing the
        computed mean and reverse (1 / x) standard deviation that are useful for
        computing gradients in backward modules.
        """
        # ATen forward layer norm additionally returns the mean and standard
        # deviation that the PyTorch
        #
        #     torch.layer_norm(input, self.normalized_shape, weight, bias, self.eps)
        #
        # wrapper hides. We want those too so we can save them for backward.
        if self.use_aten:
            output, mean, rstd = torch.ops.aten.native_layer_norm(
                input, self.normalized_shape, weight, bias, self.eps
            )
        else:
            # Compute mean and rstd with the forwarded dtype since ATen does that too.
            mean = input.to(dtype=self.forwarded_args_dtype).mean(
                dim=self.normalized_dim, keepdim=True
            )
            shift = input - mean
            var = (shift**2).mean(dim=self.normalized_dim, keepdim=True)
            rstd = torch.rsqrt(var + self.eps)
            # rstd = torch.rsqrt(input.to(self.forwarded_args_dtype).var(dim=self.normalized_dim, keepdim=True) + self.eps)
            output = (input - mean.to(dtype=input.dtype)) * rstd.to(dtype=input.dtype)
            if weight is not None:
                output *= weight
            if bias is not None:
                output += bias

        return (
            output,
            mean.to(dtype=self.forwarded_args_dtype),
            rstd.to(dtype=self.forwarded_args_dtype),
        )


class LayerNormBackwardInput(torch.nn.Module):
    """Module computing, as its forward computation, the gradient of the layer
    norm input given that of its output."""

    def __init__(self, signature: LayerNormSignature):
        super().__init__()
        self.normalized_shape = signature.normalized_shape
        self.eps = signature.eps
        self.dtype = signature.dtype

        self.normalized_dim = list(
            range(len(signature.input_shape))[-len(self.normalized_shape) :]
        )
        self.keep_dim = list(
            range(len(signature.input_shape))[: -len(self.normalized_shape)]
        )
        self.use_aten = signature.use_aten

    def forward(
        self,
        grad_output: torch.Tensor,
        input: torch.Tensor,
        weight: torch.Tensor | None,
        mean: torch.Tensor,
        rstd: torch.Tensor,
    ) -> torch.Tensor:
        if self.use_aten:
            return torch.ops.aten.native_layer_norm_backward(
                grad_output,
                input,
                self.normalized_shape,
                mean,
                rstd,
                weight,
                None,
                (True, False, False),
            )[0]

        # Recompute norm instead of saving it. Judging by the signature, this is the same
        # decision as ATen.
        mean = mean.to(dtype=self.dtype)
        rstd = rstd.to(dtype=self.dtype)
        norm = (input - mean) * rstd
        dnorm = grad_output * weight if weight is not None else grad_output
        dx = (
            dnorm
            - dnorm.mean(dim=self.normalized_dim, keepdim=True)
            - norm * (dnorm * norm).mean(dim=self.normalized_dim, keepdim=True)
        ) * rstd
        return dx


class LayerNormBackwardWeight(torch.nn.Module):
    """Module computing, as its forward computation, the gradient of the layer
    norm weight given that of its output."""

    def __init__(self, signature: LayerNormSignature):
        super().__init__()
        self.normalized_shape = signature.normalized_shape
        self.eps = signature.eps
        self.dtype = signature.dtype

        self.normalized_dim = list(
            range(len(signature.input_shape))[-len(self.normalized_shape) :]
        )
        self.keep_dim = list(
            range(len(signature.input_shape))[: -len(signature.normalized_shape)]
        )
        self.use_aten = signature.use_aten

    def forward(
        self,
        grad_output: torch.Tensor,
        input: torch.Tensor,
        weight: torch.Tensor,
        mean: torch.Tensor,
        rstd: torch.Tensor,
    ):
        if self.use_aten:
            return torch.ops.aten.native_layer_norm_backward(
                grad_output,
                input,
                self.normalized_shape,
                mean,
                rstd,
                weight,
                None,
                (False, True, False),
            )[1]

        # Recompute norm instead of saving it. Judging by the signature, this is the same
        # decision as ATen.
        mean = mean.to(dtype=self.dtype)
        rstd = rstd.to(dtype=self.dtype)
        norm = (input - mean) * rstd
        return (grad_output * norm).sum(self.keep_dim)


class LayerNormBackwardBias(torch.nn.Module):
    """Module computing, as its forward computation, the gradient of the layer
    norm bias given that of its output."""

    def __init__(self, signature: LayerNormSignature):
        super().__init__()
        self.normalized_shape = signature.normalized_shape
        self.eps = signature.eps
        self.keep_dim = list(
            range(len(signature.input_shape))[: -len(signature.normalized_shape)]
        )
        self.use_aten = signature.use_aten

    def forward(
        self,
        grad_output: torch.Tensor,
        input: torch.Tensor,
        weight: torch.Tensor,
        bias: torch.Tensor,
        mean: torch.Tensor,
        rstd: torch.Tensor,
    ) -> torch.Tensor:
        if self.use_aten:
            return torch.ops.aten.native_layer_norm_backward(
                grad_output,
                input,
                self.normalized_shape,
                mean,
                rstd,
                weight,
                bias,
                (False, False, True),
            )[2]

        return grad_output.sum(dim=self.keep_dim)


class LayerNormBackwardFull(torch.nn.Module):
    """Module computing, as its forward computation, the gradients of the input,
    weights, and bias of the layer normalization given the gradient of its
    output."""

    def __init__(self, signature: LayerNormSignature):
        super().__init__()
        self.use_aten = signature.use_aten
        self.normalized_shape = signature.normalized_shape
        self.need_bias = signature.bias
        self.need_weight = signature.elementwise_affine
        self.normalized_dim = list(
            range(len(signature.input_shape))[-len(self.normalized_shape) :]
        )
        self.keep_dim = list(
            range(len(signature.input_shape))[: -len(signature.normalized_shape)]
        )

    def forward(
        self,
        grad_output: torch.Tensor,
        input: torch.Tensor,
        weight: torch.Tensor | None,
        bias: torch.Tensor | None,
        mean: torch.Tensor,
        rstd: torch.Tensor,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        assert self.need_weight != (
            weight is None
        ), "Weight must be provided if its gradient is requested."
        assert self.need_bias != (
            bias is None
        ), "Bias must be provided if its gradient is requested."
        if self.use_aten:
            return torch.ops.aten.native_layer_norm_backward(
                grad_output,
                input,
                self.normalized_shape,
                mean,
                rstd,
                weight,
                bias,
                (True, self.need_weight, self.need_bias),
            )

        # Recompute norm instead of saving it. Judging by the signature, this is the same
        # decision as ATen.
        norm = (input - mean) * rstd
        # norm = norm.to(dtype=input.dtype)
        dnorm = grad_output * weight if weight is not None else grad_output
        dx = (
            dnorm
            - dnorm.mean(dim=self.normalized_dim, keepdim=True)
            - norm * (dnorm * norm).mean(dim=self.normalized_dim, keepdim=True)
        ) * rstd
        # dx = dx.to(dtype=input.dtype)
        dw = None
        if self.need_weight:
            dw = (grad_output * norm).sum(self.keep_dim)
        db = None
        if self.need_bias:
            db = grad_output.sum(dim=self.keep_dim)
        return dx, dw, db


def _parse_shape(shape: str) -> list[int]:
    for symbol in shape:
        assert symbol in "0123456789x", "Unsupported shape syntax."

    return list(map(int, shape.split("x")))


class _DTypeCommandDispatcher:
    SUPPORTED = {
        "layernorm": torch.float,
        "layernormfp16": torch.float16,
        "layernormbfp16": torch.bfloat16,
    }

    @staticmethod
    def choices() -> list[str]:
        return list(_DTypeCommandDispatcher.SUPPORTED.keys())

    @staticmethod
    def get_dtype(command: str) -> torch.dtype:
        assert (
            command in _DTypeCommandDispatcher.SUPPORTED
        ), f"Unknown command {command}."
        return _DTypeCommandDispatcher.SUPPORTED[command]


class LayerNormParser(OpCLIParser):
    def get_signature(args: argparse.Namespace) -> LayerNormSignature:
        shape = _parse_shape(args.input)
        # Apparently the MIOpen driver can only normalize one dimension, and seems
        # to be imprecise if it is not the last dimension.
        assert (
            args.normalized_dim == len(shape) - 1
        ), "Can only normalize one trailing dimension for now (MIOpen limitation)."
        normalized_shape = shape[args.normalized_dim :]

        try:
            mode = Mode(args.forw)
        except Exception as e:
            raise ValueError(f"Unsupported mode {args.forw}.") from e

        return LayerNormSignature(
            input_shape=shape,
            normalized_shape=normalized_shape,
            eps=args.eps,
            elementwise_affine=(args.mode == 0),
            bias=True,
            dtype=_DTypeCommandDispatcher.get_dtype(args.command),
            mode=mode,
            use_aten=args.use_aten,
        )

    def get_miopen_parser() -> argparse.ArgumentParser:
        parser = argparse.ArgumentParser()
        parser.add_argument(
            "command", default="layernorm", choices=_DTypeCommandDispatcher.choices()
        )
        parser.add_argument(
            "--forw",
            "-F",
            type=int,
            default=1,
            help="Kind of kernel to run, not compatible with MIOpen (1 forward, 2 backward input, 4 backward weight, 8 backward bias, 14 full backward)",
        )
        parser.add_argument(
            "--input",
            "-X",
            type=str,
            help="Input Tensor descriptor.\nFormat: NxC[xD]xHxW",
        )
        parser.add_argument("--eps", "-e", type=float, default=1e-5, help="Alpha")
        parser.add_argument(
            "--mode",
            "-m",
            type=int,
            default=0,
            choices=[0, 1],
            help="elemwise affine mode (0), weight and bias mode (1)",
        )
        parser.add_argument(
            "--normalized_dim", "-o", type=int, default=3, help="Normalized dim"
        )
        parser.add_argument(
            "--use-aten",
            type=bool,
            default=True,
            help="Use core ATen op instead of a manual implementation",
        )
        return parser

    @classmethod
    def get_op_name(cls) -> str:
        return "layernorm"
